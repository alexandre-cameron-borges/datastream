# Datastream (POC) ‚Äî Kafka ‚Üí Airflow ‚Üí Spark ‚Üí Elasticsearch/Kibana

> Pipeline √©v√®nementiel simple pour d√©montrer ingestion, enrichissement et recherche/visualisation.

---

## üöÄ TL;DR (Quickstart local)
1) **Pr√©requis**: Docker Desktop + Compose, Python 3.10+, Git.  
2) **Clone**: `git clone <URL> && cd datastream`  
3) **Infra**: copier `infra/.env.example` ‚Üí `infra/.env`, puis  
   `docker compose -f infra/docker-compose.yml up -d`  
4) **Topics**: cr√©er `rides_raw` & `rides_processed` (Kafka-UI ou script).  
5) **Producteur**:  
   ```bash
   python -m venv .venv && source .venv/bin/activate
   pip install -r producer/requirements.txt
   python producer/producer.py
   ```
6) **Airflow UI**: http://localhost:8081 (admin/admin) ‚Üí activer `rides_enrichment`, `es_sync`.  
7) **Kibana**: http://localhost:5601 ‚Üí index pattern `rides*`.

---

## üóÇÔ∏è Structure du repo (minimaliste)
```
datastream/
‚îú‚îÄ infra/                 # Docker & config (Kafka, ES, Kibana, Airflow)
‚îÇ  ‚îú‚îÄ .env.example
‚îÇ  ‚îî‚îÄ docker-compose.yml
‚îú‚îÄ airflow/
‚îÇ  ‚îî‚îÄ dags/               # DAGs Airflow (Python)
‚îú‚îÄ jobs/                  # Scripts Spark/Python appel√©s par Airflow
‚îú‚îÄ producer/              # G√©n√©rateur d‚Äô√©v√©nements Kafka
‚îú‚îÄ docs/                  # Guides (runbooks, sch√©mas)
‚îú‚îÄ .gitignore
‚îî‚îÄ README.md
```

**.gitignore** (extrait recommand√©) :
```
__pycache__/
*.pyc
*.log
.venv/
infra/.env
airflow/logs/
infra/datastream-sa-key.json
```

---

## üß† Architecture (vue d‚Äôensemble)
```
Producer(JSON) ‚Üí Kafka (rides_raw)
      ‚îî‚îÄ(Airflow DAG)‚Üí Spark enrichit ‚Üí rides_processed
                         ‚îî‚îÄ(Airflow DAG)‚Üí Elasticsearch ‚Üí Kibana
```
**Option cloud (plus tard)** : Kafka ‚Üí GCS (Parquet) ‚Üí BigQuery/ES.

---

## üìú Contrat de donn√©es (Data Contract)

**Topic source** `rides_raw` (JSON minimal stable) :
```json
{
  "ride_id": "uuid",
  "ts": "ISO-8601 UTC",
  "origin": {"lat": 48.85, "lon": 2.35},
  "destination": {"lat": 48.90, "lon": 2.31},
  "comfort": "eco|comfort|business",
  "duration_min": 12,
  "price_base": 3.2,
  "price_per_km": 1.2,
  "price_per_min": 0.25
}
```
**Sortie** `rides_processed` (ajouts) :
- `distance_km: double (>=0)`  
- `price_total: double = base + km*price_per_km + min*price_per_min`  

**R√®gles** : champs ci-dessus **obligatoires** ; champs inconnus ignor√©s ; JSON invalide ‚Üí **DLQ** (√† ajouter).

---

## ‚öôÔ∏è D√©ploiement local ‚Äî pas √† pas

### 1) Infra Docker
- Copier `infra/.env.example` ‚Üí `infra/.env` (exemple) :
  ```env
  # Kafka
  KAFKA_BROKER_ID=1
  KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
  KAFKA_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://0.0.0.0:29092
  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
  # Elasticsearch/Kibana
  ES_VERSION=8.14.3
  ELASTIC_PASSWORD=changeme
  # Airflow
  AIRFLOW_USER=admin
  AIRFLOW_PASSWORD=admin
  ```
- Lancer :
  ```bash
  docker compose -f infra/docker-compose.yml up -d
  ```
- V√©rifs rapides :  
  Kafka-UI: http://localhost:8080 ‚Ä¢ Kibana: http://5601 ‚Ä¢ Airflow: http://8081

### 2) Topics Kafka
- Cr√©er `rides_raw`, `rides_processed` via Kafka-UI ou script (docs/Runbook).

### 3) Producteur (√©chantillonnage de donn√©es)
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r producer/requirements.txt
python producer/producer.py
```
‚Üí Messages visibles dans Kafka-UI (topic `rides_raw`).

### 4) Airflow (DAGs)
- UI: http://localhost:8081 (admin/admin)  
- **Connections** : `kafka_local` (kafka:9092), `es_local` (http://elastic:changeme@elasticsearch:9200)  
- **Variables** : `KAFKA_TOPIC_RAW=rides_raw`, `KAFKA_TOPIC_PROCESSED=rides_processed`  
- Activer les DAGs : `rides_enrichment` ‚Üí `es_sync`.

### 5) Kibana (visualisation)
- Cr√©er **index pattern** `rides*`.  
- Graphs de base : volume d‚Äô√©v√©nements, `avg(distance_km)`, `avg(price_total)` par minute.

---

## üîç Observabilit√© & Qualit√© (POC)
- **Retries/SLA** : `default_args` des DAGs (retries=2, SLA 15 min).  
- **Logs** : Airflow UI + logs conteneurs.  
- **DLQ** : pr√©voir `rides_dlq` + op√©rateur d‚Äôenvoi en cas de parse KO.  
- **Smoke tests** : petit script qui compare nb events `raw` vs `processed`.

---

## üîê S√©curit√© (POC vs Prod)
- **POC** : mots de passe par d√©faut, pas de TLS, utilisateurs minimes.  
- **Prod (hors scope repo)** :  
  - Secrets via vault (GitHub Secrets/HashiCorp), **TLS** Kafka/ES, **RBAC** Airflow,  
  - R√©seaux Docker isol√©s, **scan d‚Äôimages**, **CI/CD** avec scans SAST/DAST.

---

## ‚òÅÔ∏è GCP ‚Äî travaux **hors GitHub** (quand on bascule cloud)
1) **Projet & CLI** :  
   ```bash
   gcloud init
   gcloud config set project <PROJECT_ID>
   gcloud services enable storage.googleapis.com bigquery.googleapis.com
   ```
2) **Stockage** : `gsutil mb -l EU gs://<BUCKET_NAME>/`  
3) **BigQuery** : dataset `rides` (partitionn√© par date).  
4) **Service Account** : `datastream-sa` (r√¥les `storage.admin`, `bigquery.admin`) + cl√© JSON.  
5) **Airflow (cloud)** : monter la cl√© dans le worker (ou Secret Manager) + `GOOGLE_APPLICATION_CREDENTIALS`.  
6) **Flux cible recommand√©** :  
   - Kafka ‚Üí **GCS (Parquet)** via Spark/Kafka Connect (batch micro-batch)  
   - GCS ‚Üí **BigQuery** (tables partitionn√©es/clusteris√©es)  
   - (Option) BigQuery/Parquet ‚Üí **Elasticsearch** batch√© pour recherche rapide  
7) **Co√ªts & gouvernance** : lifecycle GCS, labels, IAM minimal, quotas BQ, VPC-SC (si sensible).

---

## üß≠ Roadmap
- **v0** : POC local fonctionnel (ce repo)  
- **v1** : DLQ + **Schema Registry** (Avro/JSON-Schema)  
- **v2** : Sink **GCS/BigQuery** + jobs batch  
- **v3** : Observabilit√© avanc√©e (OpenLineage, m√©triques), **CI/CD**  
- **v4** : S√©curit√© & HA (TLS, RBAC, autoscaling)

---

## üõ†Ô∏è D√©pannage rapide
- **Ports occup√©s** ‚Üí modifier `infra/.env` (ports).  
- **DAGs non vus** ‚Üí v√©rifier montages, timestamp fichiers, `AIRFLOW__CORE__LOAD_EXAMPLES=False`.  
- **ES vide** ‚Üí relancer `es_sync`, checker logs Airflow et statut cluster ES.  
- **Producer en erreur** ‚Üí v√©rifier `KAFKA_ADVERTISED_LISTENERS` et port `29092` c√¥t√© host.

---

## üìé Annexes
- **Make (optionnel)** : ajoute un `Makefile` avec `make up`, `make down`, `make topics`, `make clean`.  
- **ADR** : documente les choix (`docs/adr/0001-...md`).
